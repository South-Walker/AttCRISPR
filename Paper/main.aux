\relax 
\citation{jinek2012a}
\citation{cong2013multiplex}
\citation{mali2013rnaguided}
\citation{rubeis2018risks}
\citation{kang2016introducing}
\citation{ishii2017reproductive}
\citation{liang2015crispr/cas9-mediated}
\citation{ishii2017reproductive}
\citation{slaymaker2016rationally}
\citation{kleinstiver2016high-fidelity}
\citation{wang2019optimized}
\citation{chuai2018deepcrispr}
\citation{liu2019computational}
\newlabel{^_1}{{}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\citation{Song2020}
\citation{kim2018deep}
\citation{kim2019spcas9}
\citation{MuhammadRafid2020}
\citation{MuhammadRafid2020}
\citation{Liu2019}
\citation{Liu2019}
\citation{wang2019optimized}
\citation{wang2019optimized}
\citation{lundberg2017a}
\citation{slack2019fooling}
\citation{wang2019optimized}
\citation{Liu2019}
\citation{MuhammadRafid2020}
\newlabel{eq:01}{{1}{2}}
\newlabel{eq:02}{{2}{2}}
\newlabel{eq:03}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two categories of the deep learning models used in sgRNA related task. (a) Model work in spatial domain. In spatial domain, the base sequence is encoded into a binary matrix (or a binary image). Since convolution has great advantages in extracting spatial features, the convolutional neural network is an excellent tool in spatial domain. (b) Model work in temporal domain. In temporal domain, the base sequence (represented by the binary matrix) is embedded into a sequance of high-dimensional vector, in which the recurrent neural network perform better. In addition, we note that the last layers of neural network are usually full connection structure (not necessarily), which greatly increases the difficulty of understanding the decisions of model. }}{2}\protected@file@percent }
\newlabel{fig:01}{{1}{2}}
\newlabel{eq:04}{{4}{2}}
\newlabel{eq:05}{{5}{2}}
\newlabel{eq:07}{{6}{2}}
\citation{liu2019computational}
\citation{lin2018off-target}
\citation{kim2018deep}
\citation{chuai2018deepcrispr}
\citation{zhang2020c-rnncrispr:}
\citation{zhang2020c-rnncrispr:}
\citation{huang2018improving}
\citation{wang2019optimized}
\citation{liu2020deep}
\citation{Liu2019}
\citation{Liu2019}
\citation{Liu2019}
\citation{chaudhari2019an}
\citation{wang2014genetic}
\citation{wu2014genome-wide}
\citation{Liu2019}
\citation{Liu2019}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and methods}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.1}}Sequence encoding and embedding}{3}\protected@file@percent }
\newlabel{eq:08}{{7}{3}}
\citation{woo2018cbam:}
\citation{cong2013multiplex}
\citation{jinek2012a}
\citation{mali2013rnaguided}
\citation{vaswani2017attention}
\citation{luong2015effective}
\citation{bahdanau2014neural}
\citation{vaswani2017attention}
\citation{luong2015effective}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The architecture of spatial domain method in AttCRISPR. The input of the method is encoded sgRNA sequence $X_{oh}$, a $21\times 4$ one-hot matrix. Then refine it through a spatial attention module, which could tell us the importance of a specific matrix element (or just say, pixel). A simple convolutional neural network followed is applied to extract potential feature representation of sgRNA sequence. In the last step, we flatten the output of convolutional neural network into a one dimensional vector and use a multilayer perceptron with sigmoid activation function to achieve the spatial output $y_s$.}}{4}\protected@file@percent }
\newlabel{fig:02}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Details of spatial attention module. A convolution layer is used to generate multi-channel map from $X_{oh}$. Then concatenated the output of both max-pooling and average-pooling method and forward it to the last convolution layer. A sigmoid function is used to map the final result to a range of zero to one at last, which generates the spatial first-order preference matrix $A_s$. }}{4}\protected@file@percent }
\newlabel{fig:03}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.2}}Neural network architecture}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{2.2}.1}}Method and attention in spatial domain}{4}\protected@file@percent }
\newlabel{eq:11}{{8}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{2.2}.2}}Method and attention in temporal domain}{4}\protected@file@percent }
\newlabel{eq:13}{{9}{4}}
\newlabel{eq:14}{{10}{4}}
\citation{wang2019optimized}
\citation{Li2016ProteinSS}
\citation{Wang2016ProteinSS}
\citation{wang2019optimized}
\citation{wang2019optimized}
\citation{wang2019optimized}
\citation{wang2019optimized}
\citation{MuhammadRafid2020}
\newlabel{eq:16}{{11}{5}}
\newlabel{eq:17}{{12}{5}}
\newlabel{eq:18}{{13}{5}}
\newlabel{eq:19}{{14}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{2.2}.3}}Model ensemble following stacking strategy}{5}\protected@file@percent }
\newlabel{eq:20}{{15}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.3}}Datasets and current prediction method}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The architecture of temporal domain method in AttCRISPR. The input of the method is embedding sgRNA sequence $X_e$, a $21\times e$ embedding matrix, where $e$ is the dimension of a nucleotide embedding vector. Then Keys $K$, Values $V$ and Queries $Q$ is generated through a classic encoder-decoder structure which is need for temporal attention module. Next, the temporal attention module generates the first-order preference $\tilde  {A}$, a $21\times e$ matrix (or a vector set). Each of the row vectors in matrix $\tilde  {A}$ represents the base preference of sgRNA at the corresponding position, we use their dot product with the corresponding row vector in embedded $X_e$ to build the score of corresponding position. Hence, a full connection layer is used to weighted average them and achieve the temporal output $y_t$. }}{5}\protected@file@percent }
\newlabel{fig:04}{{4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Details of temporal attention module. To generate the first order preference vector in the $i$ position of sgRNA $\tilde  {A}_i$. First, the $i$-th row vector of the querys matrix $Q_i$ is multiplied by the transpose of the keys mateix $K^T$, and apply a softmax function to obtain a preliminary weights vector on the values. Second, to favor the alignment points near $i$, the weights vector obtained is multiplied element-by-element by the $i$-th row vector of the damping matrix $G$, as Equation\nobreakspace  {}(12\hbox {}) has shown. $G_{ij}$ can be regarded as the result of place a Gaussian distribution centered around $i$, then sampling the position $j$ (a scaling factor is used to ensure the sum of $G_I$ is $l$). Then we achieve the second-order preference matrix $B$, it is also a weights matrix on the values. So the first-order preference matrix $\tilde  {A}$ come from the product of $B$ and values matrix $V$. Although it's not shown above, it is also worth noting that, a full connection layer with L2 constraints is used to generates a transformation matrix in order to ensure $\tilde  {A}_i$ and ${X_e} _i$ are in the same vector space. Temporal attention module generates the temporal first-order preference matrix $\tilde  {A}$ and the temporal second-order preference matrix $B$. }}{5}\protected@file@percent }
\newlabel{fig:05}{{5}{5}}
\citation{wang2019optimized}
\citation{wang2019optimized}
\citation{MuhammadRafid2020}
\citation{wang2019optimized}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The methods compared in our work and brief description of these methods}}{6}\protected@file@percent }
\newlabel{Tab:01}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.4}}Experiments}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces In the absence of hand-crafted biological features, performance of different algorithms for sgRNA activity prediction. (a)-(c) The performance of Temporal AttCRISPR, Spatial AttCRISPR and Ensemble AttCRISPR. The half-violin plots show the mean and distribution of the Spearman correlation coefficient between predicted and measured sgRNA activity scores over all tests. (e)-(g) In the absence of hand-crafted biological features, the performance of all prediction methods in these datasets as far as we know. The $mean \pm s.d.$ of the Spearman correlation coefficient between predicted and measured sgRNA activity scores are shown in the bar plots.}}{6}\protected@file@percent }
\newlabel{fig:06}{{6}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.1}}Model building and stacking}{6}\protected@file@percent }
\newlabel{section:stacking}{{{3.1}}{6}}
\citation{wang2019optimized}
\citation{MuhammadRafid2020}
\bibstyle{natbib}
\bibdata{document}
\bibcite{bahdanau2014neural}{{1}{2014}{{Bahdanau {\em  et~al.}}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{chaudhari2019an}{{2}{2019}{{Chaudhari {\em  et~al.}}}{{Chaudhari, Polatkan, Ramanath, and Mithal}}}
\bibcite{chuai2018deepcrispr}{{3}{2018}{{Chuai {\em  et~al.}}}{{Chuai, Ma, Yan, Chen, Hong, Xue, Zhou, Zhu, Chen, Duan, {\em  et~al.}}}}
\bibcite{cong2013multiplex}{{4}{2013}{{Cong {\em  et~al.}}}{{Cong, Ran, Cox, Lin, Barretto, Habib, Hsu, Wu, Jiang, Marraffini, {\em  et~al.}}}}
\bibcite{huang2018improving}{{5}{2018}{{Huang {\em  et~al.}}}{{Huang, Zhao, Dou, Wen, and Chang}}}
\bibcite{ishii2017reproductive}{{6}{2017}{{Ishii}}{{Ishii}}}
\bibcite{jinek2012a}{{7}{2012}{{Jinek {\em  et~al.}}}{{Jinek, Chylinski, Fonfara, Hauer, Doudna, and Charpentier}}}
\bibcite{kang2016introducing}{{8}{2016}{{Kang {\em  et~al.}}}{{Kang, He, Huang, Yu, Chen, Gao, Sun, and Fan}}}
\bibcite{kim2018deep}{{9}{2018}{{Kim {\em  et~al.}}}{{Kim, Min, Song, Jung, Choi, Kim, Lee, Yoon, and Kim}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Performance comparisons for different methods in the absence of hand-crafted biological features (take Spearman correlation coefficient as evaluation index)}}{7}\protected@file@percent }
\newlabel{Tab:02}{{2}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Performance comparisons for methods before and after integrating with hand-crafted biological features (take Spearman correlation coefficient as evaluation index)}}{7}\protected@file@percent }
\newlabel{Tab:03}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.2}}Comparison to current methods}{7}\protected@file@percent }
\newlabel{section:comparison}{{{3.2}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Performance comparisons for methods before and after integrating with hand-crafted biological features, where DeepHF is the RNN integrated with hand-crafted biological features, and StAC is the EnAC integrated with hand-crafted biological features. The box plot show the mean and distribution of Spearman correlation coefficient between predicted and measured sgRNA activity scores over all tests. }}{7}\protected@file@percent }
\newlabel{fig:07}{{7}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.3}}Interpretability of the AttCRISPR}{7}\protected@file@percent }
\newlabel{section:interpretability}{{{3.3}}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{7}\protected@file@percent }
\bibcite{kim2019spcas9}{{10}{2019}{{Kim {\em  et~al.}}}{{Kim, Kim, Lee, Min, Bae, Choi, Park, Jung, Yoon, and Kim}}}
\bibcite{kleinstiver2016high-fidelity}{{11}{2016}{{Kleinstiver {\em  et~al.}}}{{Kleinstiver, Pattanayak, Prew, Tsai, Nguyen, Zheng, and Joung}}}
\bibcite{Li2016ProteinSS}{{12}{2016}{{Li and Yu}}{{Li and Yu}}}
\bibcite{liang2015crispr/cas9-mediated}{{13}{2015}{{Liang {\em  et~al.}}}{{Liang, Xu, Zhang, Ding, Huang, Zhang, Lv, Xie, Chen, Li, {\em  et~al.}}}}
\bibcite{lin2018off-target}{{14}{2018}{{Lin and Wong}}{{Lin and Wong}}}
\bibcite{liu2019computational}{{15}{2019a}{{Liu {\em  et~al.}}}{{Liu, Zhang, and Zhang}}}
\bibcite{Liu2019}{{16}{2019b}{{Liu {\em  et~al.}}}{{Liu, He, and Xie}}}
\bibcite{liu2020deep}{{17}{2020}{{Liu {\em  et~al.}}}{{Liu, Cheng, Liu, Li, and Liu}}}
\bibcite{lundberg2017a}{{18}{2017}{{Lundberg and Lee}}{{Lundberg and Lee}}}
\bibcite{luong2015effective}{{19}{2015}{{Luong {\em  et~al.}}}{{Luong, Pham, and Manning}}}
\bibcite{mali2013rnaguided}{{20}{2013}{{Mali {\em  et~al.}}}{{Mali, Yang, Esvelt, Aach, Guell, Dicarlo, Norville, and Church}}}
\bibcite{MuhammadRafid2020}{{21}{2020}{{Muhammad~Rafid {\em  et~al.}}}{{Muhammad~Rafid, Toufikuzzaman, Rahman, and Rahman}}}
\bibcite{rubeis2018risks}{{22}{2018}{{Rubeis and Steger}}{{Rubeis and Steger}}}
\bibcite{slack2019fooling}{{23}{2019}{{Slack {\em  et~al.}}}{{Slack, Hilgard, Jia, Singh, and Lakkaraju}}}
\bibcite{slaymaker2016rationally}{{24}{2016}{{Slaymaker {\em  et~al.}}}{{Slaymaker, Gao, Zetsche, Scott, Yan, and Zhang}}}
\bibcite{Song2020}{{25}{2020}{{Song {\em  et~al.}}}{{Song, Kim, Lee, Kim, Seo, Park, Choi, Jang, Shin, Min, Quan, Kim, Kang, Yoon, and Kim}}}
\bibcite{vaswani2017attention}{{26}{2017}{{Vaswani {\em  et~al.}}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wang2019optimized}{{27}{2019}{{Wang {\em  et~al.}}}{{Wang, Zhang, Wang, Li, Wang, Liu, Wang, Zhou, Shi, Lan, {\em  et~al.}}}}
\bibcite{Wang2016ProteinSS}{{28}{2016}{{Wang {\em  et~al.}}}{{Wang, Peng, Ma, and Xu}}}
\bibcite{wang2014genetic}{{29}{2014}{{Wang {\em  et~al.}}}{{Wang, Wei, Sabatini, and Lander}}}
\bibcite{woo2018cbam:}{{30}{2018}{{Woo {\em  et~al.}}}{{Woo, Park, Lee, and Kweon}}}
\bibcite{wu2014genome-wide}{{31}{2014}{{Wu {\em  et~al.}}}{{Wu, Scott, Kriz, Chiu, Hsu, Dadon, Cheng, Trevino, Konermann, Chen, {\em  et~al.}}}}
\bibcite{zhang2020c-rnncrispr:}{{32}{2020}{{Zhang {\em  et~al.}}}{{Zhang, Dai, and Dai}}}
\global\@namedef{@lastpage@}{8}
