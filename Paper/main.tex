\documentclass{bioinfo}
\copyrightyear{2020} \pubyear{2020}
\usepackage{booktabs}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{Manuscript Category}

\begin{document}
\firstpage{1}

\subtitle{Sequence analysis}

\title[short Title]{AttCRISPR : an spacetime interpretable model for sgRNA efficiency prediction
}
\author[Sample \textit{et~al}.]{Corresponding Author\,$^{\text{\sfb 1,}*}$, Co-Author\,$^{\text{\sfb 2}}$ and Co-Author\,$^{\text{\sfb 2,}*}$}
\address{$^{\text{\sf 1}}$Department, Institution, City, Post Code, Country and \\
$^{\text{\sf 2}}$Department, Institution, City, Post Code,
Country.}

\corresp{$^\ast$To whom correspondence should be addressed.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\abstract{\textbf{Motivation:} More and more higher specificities Cas9 variants are developed to avoid the off-target effect, which bring a significant volume of experimental data. 
Conventional machine learning performance poorly on these datasets, while model based on deep learning are often lack of interpretability, which makes it difficult for researchers to understand its decisions. 
Moreover, neither the deep learning based model with existing structure can not satisfy enough precision at such huge datasets.\\
\textbf{Results:} To overcome it, we design and implement AttCRISPR, a deep learning based model to predict the on-target activity. 
Our model was trained and tested on the biggest dataset, DeepHF dataset, as far as we know for performance evaluation. 
AttCRISPR achieves the best performance on DeepHF dataset, yielding an average spearman value of 0.99\%, 0.99\%, 0.99\% (corresponding to WT-SpCas9, eSpCas9(1.1), SpCas9-HF1) under tenfold shuffled validation. 
In addition, another advantage of AttCRISPR over other well-perform methods is that it is intrinsic interpretable and does not rely on other post hoc explanations techniques. 
In this paper, We design a set of algorithm to reveal the biological significance of the decision maked by AttCRISPR from the global and local perspectives at sgRNA overall level and nucleotide level.\\
\textbf{Availability:} The example code are available at \href{https://github.com/South-Walker/AttCRISPR}{https://github.com/South-Walker/AttCRISPR}\\
\textbf{Contact:} \href{xlm@xiaoliming96.com}{xlm@xiaoliming96.com}\\
\textbf{Supplementary information:} Supplementary data are available at \textit{Bioinformatics}
online.}

\maketitle

\section{Introduction}

Clustered regularly interspaced short palindromic repeats (CRISPR) / CRIPSR associated protein 9 (Cas9) systems is preferred over other biological research and human medicine technologies now, beacuse of it's efficiency, 
 robustness and programmability.  Cas9 nucleases can be directed by short guide RNAs (sgRNAs) to introduce site-specific DNA double-stranded breaks in target, 
 so to enable editing site-specific within the mammalian genome \citep{jinek2012a,cong2013multiplex,mali2013rnaguided}. CRISPR/Cas9, to a large extent, has developed genetic therapies at the cellular level, 
 while there are still severe medical disadvantage even now which has greatly hindered the further clinical application of the CRISPR/Cas9 systems. 
 One of these disadvantage is due to point mutations caused by off-target effects \citep{rubeis2018risks,kang2016introducing,ishii2017reproductive,liang2015crispr/cas9-mediated}. 
 To overcome this disadvantage, a solution is to engineer CRISPR / Cas9 with higher specificities. 
 That's why more and more higher specificities Cas9 variants, such as enhanced SpCas9 (eSpCas9(1.1)), 
 Cas9-High Fidelity (SpCas9-HF1) \citep{ishii2017reproductive,slaymaker2016rationally}, hyper-accurate Cas9 (HypaCas9) \citep{kleinstiver2016high-fidelity}, 
 been developed and bring a significant volume of experimental data, that is to say researchers have to face the difficulty of analyzing such huge and heterogeneous data.

The activity of chosen sgRNA sequence determines the success of genome editing, however distinctly fluctuant behaviors have been observed for the performance of different sgRNAs, even in the same Cas9 system.
 Some optimum sgRNAs can hit almost all targers alleles, while anothers don't even show activity \citep{wang2019optimized}.
 This fact indicates that it's meaningful to explore an efficient approaches to guide sgRNA design. 


In practice, there have been a number of application and toolkit applied in this task. In the earlier studies, methods in silico are categorized into three types: 
 (1) alignment-based, (2) hypothesis-driven and (3) learning-based \citep{chuai2018deepcrispr}. 
 Recently we noticed that the last type of method seems to be getting more attention because of huger and huger data set \citep{liu2019computational}. 

Learning-based method, which designed to predict the on-target activity (or off-target probability) of sgRNAs, 
 is essentially a computational model built by machine learning algorithm, not only conventional machine learning but also deep learning algorithm. 
 In general, the single or multiple base sequences (vary according to the task) and biological features are represented as a multi-dimensional vector $X\in\mathbb{R}^d$, 
 and $d=l+b$, where $l$ refers to the length of base sequences and $b$ refers to the number of biological features, these methods can be represented as
\begin{equation}
\text{\it y}=Score(\it X)\label{eq:01}
\end{equation}
 where $Score(\cdot)$ depends on the algorithm being selected, and $y$ denotes the predicted value.
 Some studies on HT\_ABE and HT\_CBE have shown that deep learning based models often outperformed conventional machine learning, when the number of sgRNAs in the data set reached a certain level \citep{Song2020,kim2018deep,kim2019spcas9}. 
 Per contra, conventional machine learning algorithms, such as linear regression, logistic regression and the decision tree, are often more interpretable due to the fewer parameters and clearer mathematical assumptions. 
 In short, what was needed for developer is to trade-off accuracy and interpretability. 
 \citeauthor{MuhammadRafid2020} consider deep-learning models as black boxs and believe they lack interpretability, 
 motivated by the empirical assertion, they turn to build a model based conventional machine learning to compete with state-of-the-art deep learning models \citep{MuhammadRafid2020}. 
 On the other hand, input perturbation based feature importance analysis become a preferred components to reveal the importance of features in deep learning models. 
 \citeauthor{Liu2019} use a sliding window of length 2 to extract dimeric as input and rank the position of dimeric by contribution to final output \citep{Liu2019}. 
 One regret is that subject to the processing of the input sgRNA sequence, their analysis can not exactly on the nucleotide class. 
 Further, SHAP, one of the most prominent of model explain techniques, has been widely used to understand the decision made by the model. 
 \citeauthor{wang2019optimized} develope DeepHF, a deep learning based model, and use Deep SHAP to revealed nucleotide contributions \citep{wang2019optimized}. 
 Deep SHAP is a compositional approximation of SHAP values since it is challenged to computate SHAP values exactly, especially for a complex deep neural networks \citep{lundberg2017a}. 
 In our understanding, the method based on input perturbation often requires better generalization ability of the model (even for artificial ridiculous noise data). 
 Moreover, recent work indicates that model explain techniques, which based post hoc explanations techniques and input perturbations, could be fooled to generate meaningless explanations instead of reflecting the underlying biases \citep{slack2019fooling}, 
 in other word, they could be unreliable and misleading, even on model with excellent performance. 
 In addition, as far as we know, all the interpretable deep learning models today can only analyze the preference of on-target activity (or off-target probability) on specific nucleotide species and position 
 for example, the G adjacent to PAM has a positive effect on sgRNA activity as \citeauthor{wang2019optimized} report, 
 which we'll call the first-order preference in our paper, due to it's similar to the total differential of $y$ in Equation~(\ref{eq:01}) as following
\begin{equation}
dy = \sum^{d}_ia_idx_i\label{eq:02}
\end{equation}
\begin{equation}
a_i=\frac{\partial}{\partial x_i} Score(X)\label{eq:03}
\end{equation}
 where $x_i$ denotes the $i$-th dimension of the vector $X$, 
 $a_i$ indicates how dramatically the function changes as $x_i$ changes in a neighborhood of $X$. 
 Similarly the first-order preference indicates how dramatically the function changes as $x_i$ changes, in other word, the importance of $x_i$.
 However they didn't analyzed the preference for position $i$ nucleotide at the overall level, 
 which we'll call the second-order preference in our paper due to its calculation is based on first-order preference, 

 Specifically, we use a vector $A_i$ to build the first-order original preference at position $i$ within sgRNA sequence, then
 define $\tilde{A}$ as the first-order combine preference matrix (or just first-order preference), which means $\tilde{A}$ can be expressed linearly by $A$ as follow
\begin{equation}
\tilde{A}=BA\label{eq:04}
\end{equation}
where the weight matrix $B\in\mathbb{R}^{l\times l}$ is learned through attention mechanism. 
Then the predicted value can be expressed as
\begin{equation}
y=\sum_i^lW_i\tilde{A}_i\cdot {X_e}_i\label{eq:05}
\end{equation}
 where $W\in\mathbb{R}^l$ denotes a non-negative weight vector, $W_i$ denotes the weight of the $i$-th position, ${X_e}_i$ denotes the embedding vector of the nucleotide at $i$-th position.
 Assume that in Equation~(\ref{eq:04}) the first-order original preference at each position is relatively independent and weight matrix $B$ is independent of any first-order original preference, 
 then the total differential of the first-order preference at $i$-th position $\tilde{A}_i$ could be written as
\begin{equation}
d\tilde{A}_i=\sum_j^lB_{ij}EdA_j\label{eq:07}
\end{equation}
where $E$ is the identity matrix, and we call $B$ as the second-order preference matrix, 
it can explain how a particular pattern containing two nucleotides affects the base sequence, 
for example, how does the degree of preference for the G adjacent to PAM be affected by a T at 2 bases upstream?

\begin{figure}[!tpb]%figure2
    \centerline{\includegraphics[width=86mm]{category.png}}
    \caption{Two categories of the deep learning models used in sgRNA related task. 
    (a) Model work in spatial domain. 
    In spatial domain, the base sequence is encoded into a binary matrix (or a binary image). 
    Since convolution has great advantages in extracting spatial features, the convolutional neural network is an excellent tool in spatial domain. 
    (b) Model work in temporal domain. 
    In temporal domain, the base sequence (represented by the binary matrix) is embedded into a sequance of high-dimensional vector, 
    in which the recurrent neural network perform better.
    In addition, we note that the last layers of neural network are usually full connection structure (not necessarily), 
    which greatly increases the difficulty of understanding the decisions of model.
    }\label{fig:01}
\end{figure}
 The previous work (even based on conventional machine learning algorithms with high interpretability) 
 was limited to dimers which can only work on the adjacent base pairs \citep{Liu2019} 
 or have to perform complex feature engineering \citep{MuhammadRafid2020}.
 In light of the above, we believe it is essential to develope a model which can not only match deep learning based model in performance, 
 but also be comparable to conventional machine learning algorithms in interpretability.

Deep neural network has shown its power in the study of CRISPR/Cas9 and its improved Systems \citep{liu2019computational}. 
 Most of the deep neural network existing are the combination of recurrent neural network (RNN), convolutional neural network (CNN), fully connected neural network (FNN), and their variants. 
 As Figure~1\vphantom{\ref{fig:01}} show,  
 We found that the deep learning models used in sgRNA on-target activity (even for off-target effect) prediction tasks in recent years can be divided into the following two categories according to the encoding approach of the sgRNA sequence (sgRNA-DNA sequence pair, for off-target effect prediction):

\begin{enumerate}
    \item Methods in spatial domain. 
    Some previous studies have used model based CNN to predict gRNA on-target activity or off-target effects \citep{lin2018off-target,kim2018deep,chuai2018deepcrispr}. 
    They process sgRNA base sequence inputs with the help of one-hot encoding idea. 
    In other words, they regard it as two-dimensional image data, and use convolution layer to extracte potential features in spatial domain, 
    It is worth noting that \citeauthor{zhang2020c-rnncrispr:} adds bidirectional gated recurrent unit (BGRU, in short), a RNN variant, after pooling layer of classic CNN network \citep{zhang2020c-rnncrispr:}. 
    Our explanation is that BGRU assists CNN to extract spatial features in one dimension, under this belief it belong to this category. 
    \item Methods in temporal domain. 
    Although RNN-based network have been shown effective to improve the performance of the model with temporal sequential input, especially in natural language processing and sequential recommendation\citep{huang2018improving}, 
    RNN be not used for gRNA activity prediction, until recently \citep{wang2019optimized,liu2020deep,Liu2019}. 
    They consider the nucleotides(can also dimer or polymer) in the sgRNA sequence as word, and the sgRNA sequence itself as a sentence (from 5' to 3'), then a trainable matrix (could be either supervised or unsupervised) is used to project the word to the dense real-valued space. 
    This technology is called embedding, which generates the base embedding. RNN further encoding the base embedding into a sequence of hidden state vector. 
    Specially, \citeauthor{Liu2019} use RNN and CNN in parallel to extract features in base embedding \citep{Liu2019}. 
    However, base embedding is not spatially interpretable (different from one-hot encode), and they have no way to further explore the correlation between CNN and RNN output. 
    Almost all RNN based models used in sgRNA on-target activity or off-target effect flatten the hidden state vector into a one-dimensional vector as the input of the fully connected layer. 
    It is a pity that the temporal sequential dependency of hidden state vector are rarely noticed. 
    To summarise, RNN has limited representation power in capturing spatial feature. Furthermore, hidden state vector representation is usually hard to understand and explain.
\end{enumerate}

Attention mechanism has demonstrated its power in Natural Language Processing, Statistical Learning, Speech and Computer Vision \citep{chaudhari2019an}. 
 It makes model tends to focus selectively on parts of the input which is help in performing the task effectively. 
 Previous observation have shown that Cas9 preferentially binds sgRNAs containing purines but not pyrimidines \citep{wang2014genetic} and multiple thymine in the spacer impairing sgRNA activity \citep{wu2014genome-wide}, 
 that is to say some specific nucleotide and base position need more attention compared to others. 
 The above is the premise of introducing attention mechanism. Strictly speaking, we are not the first to bring attention mechanisms into this field. 
 The most similar approach to ours is the work based on transformer by \citeauthor{Liu2019}. 
 They use transformer, a components based on attention mechanism, instead of RNN to improve the ability of temporal feature extraction, 
 hence, enhance the performance of their model \citep{Liu2019,vaswani2017attention}. 
 In our work,the interpretability benefit from attention mechanism is more focused.
 In this paper, our main contributions are as follows:\vspace*{1pt}
\begin{itemize}
    \item Present a novel deep-learning model, which can extract potential feature representation of sgRNA sequence in both spatial and temporal domain parallelly. 
    Finally, the ensemble learning method is used to combine the two to achieve better performance than other models.
    It doesn't belong to any of the above categories of existing approaches.
    \item Introduce attention mechanism into our model. 
    As a result, It does not need post hoc explanations techniques based on input perturbation to explain itself. 
    It is intrinsic interpretable in both temporal and spatial domains.
    In the spatial domain it's at the nucleotide level, in our word, first-order preference, 
    while at the overall level in the temporal domain, in our word, second-order preference. 
    Thus, it is transformed from a black box to an intrinsically interpretable model with the performance of deep learning based model. 
    \item Throught ablation analysis and testing a series of possible network structure, 
    we find there are multiple components and strategy can improve the performance of our model and constructed AttCRISPR, 
    which could outperform the current state-of-the-art tool on DeepHF dataset.\vspace*{1pt}
\end{itemize}

\section{Materials and methods}

\subsection{Sequence encoding and embedding}

For encoding process, we use the complementary base to represent the original base in sgRNA, as others do. 
Further, we use one-hot encode strategy, which's is to say, we encoding each base in sgRNA into a four-dimensional vector 
(encode A,T,G,C into [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1], respectively), called one-hot vector. 
Then a sgRNA can be considered as a matrix $X_{oh}\in\mathbb{R}^{l\times4} $, named one-hot matrix (a little sparse, since an one-hot vector is zero in all but one dimension). 
We believe it is meaningful to regard $X_{oh}$ as a binary image, 
therefore, it is used as an input of convolutional neural network, which performs well in the image field.
Meanwhile, as mentioned above, one-hot matrix is a little sparse. 

To facilitate the training process, we can mapping each one-hot vector into a dense real-valued high-dimensional space, which is called embedding. 
Concretely speaking, at the matrix level, the formula is as follows
\begin{equation}
X_e=X_{oh}E_m\label{eq:08}
\end{equation}
where $X_e$ named embedding matrix, $E_m\in\mathbb{R}^{4\times m}$ is a trainable transformational matrix, $m$ refers to the dimension of embedding space. 
We believe it is also meaningful to regard nucleotides in the sgRNA sequence as word, and the sgRNA sequence itself as a sentence, 
Guided by this belief $E_m$ is the word embedding matrix and $X_e$ is the sentence embedding in natural language processing (NLP).
Therefore, $X_e$ is used as an input of recurrent neural network (or its variant), which performs well in the NLP field.
In the area of NLP, there have been many methods based on pre-training and unsupervised learning been developed to determine the $E_m$. 
However, we believe that sgRNA sequence is not a natural language in the true sense, the method above does not apply to it. 
So $E_m$ will be trained along with the model as a whole. 

Due to each element of $X_{oh}$ is interpretable (representing whether there is a corresponding nucleotide type at the corresponding location), 
we call $X_{oh}$ the spatial input, and the convolutional neural network work on $X_{oh}$ is method in spatial domain. 
In other hand, different from $X_{oh}$, $X_e$ can only be explained in the first dimension (representing the embedding vector of corresponding nucleotide type), 
and embedding vector is difficult for humans to understand. 
That's why we call $X_e$ the temporal input, and the recurrent neural network (or its variant) work on $X_e$ is method in temporal domain. 

\subsection{Neural network architecture}

Based on the categorization above, we assume that method in spatial domain and in temporal domain are heterogeneous, 
which can satisfy the diversity premise of ensemble learning. 
Based on assumption above and ensemble learning, we follow the stacking strategy to develop AttCRISPR 
which can extract potential feature representation of sgRNA sequence in both spatial and temporal domain parallelly. 
Further, we apply attention mechanisms in both spatial and temporal domain respectively to enhance the interpretability of AttCRISPR.

\subsubsection{Method and attention in spatial domain}

\begin{figure}[!tpb]%figure2
    \centerline{\includegraphics[width=86mm]{CNNv2.png}}
    \caption{The architecture of spatial domain method in AttCRISPR. 
    The input of the method is encoded sgRNA sequence $X_{oh}$, a $21\times 4$ one-hot matrix. 
    Then refine it through a spatial attention module, which could tell us the importance of a specific matrix element (or just say, pixel).
    A simple convolutional neural network followed is applied to extract potential feature representation of sgRNA sequence.
    In the last step, we flatten the output of convolutional neural network into a one dimensional vector and 
    use a multilayer perceptron with sigmoid activation function to achieve the spatial output $y_s$.}\label{fig:02}
\end{figure}
As Figure~2\vphantom{\ref{fig:02}} show, method in spatial domain relies on the convolutional neural network. 
As previously mentioned, sgRNA sequence has been encoded into a $21\times 4$ one-hot matrix $X_{oh}$, and we regard $X_{oh}$ as a binary image. 
Then, convolution kernels with different size are used to extract potential spatial features just like what other do in computer vision. 
According to the foregoing, the Spatial Attention Module proposed by \citeauthor{woo2018cbam:} can be applied in our method in spatial domain, 
which was used to improve the performance of CNN in vision tasks.

\begin{figure}[!tpb]%figure3
    \centerline{\includegraphics[width=86mm]{spatialmodule.png}}
    \caption{Details of spatial attention module. A convolution layer is used to generate multi-channel map from $X_{oh}$.
    Then concatenated the output of both max-pooling and average-pooling method and forward it to the last convolution layer.
    A sigmoid function is used to map the final result to a range of zero to one at last, 
    which generates the spatial first-order preference matrix $A_s$.
    }\label{fig:03}
\end{figure}

As Figure~3\vphantom{\ref{fig:03}} show, Given an one-hot map $X_{oh}$, spatial attention module generating spatial attention matrix 
$A_s\in\mathbb{R}^{l\times 4}$ with the same as $X_{oh}$ in shape. 
Each element of $A_s$ is constrained to a range of zero to one, implemented by a sigmoid function, which reflects the importance of the corresponding elements of $X_{oh}$.
The overall spatial attention process can be summarized as:
\begin{equation}
\left\{\begin{array}{l}
X_{mc} = f^{3\times4}(X_{oh})
\\A_s = \sigma(f^{3\times2}([AvgPool(X_{mc});MaxPool(X_{mc})]))
\\X_{rf} = A_s\otimes X_{oh}
\end{array}\right.\label{eq:11}
\end{equation}
where $f^{p\times q}$ represents a convolution operation with the filter size of $p\times q$, 
$p,q\in\mathbb{Z}^{+}$, $X_{mc}$ is a multi-channel map generated by $X_{oh}$, $\sigma$ denotes the sigmoid function, 
AvgPool denotes the average-pooling operation, MaxPool denotes the max-pooling operation, $\otimes$ denotes element-wise multiplication.
Spatial attention matrix $A_s$ formally conforms to our proposed definition of first-order preference (each element of $X_{oh}$ is multiplied by the corresponding element of $A_s$), 
in other word, element of $A_s$ reveal how important the corresponding elements in $X_{oh}$ is. 
We think it can reveal the preference of the scoring function at each position. 
For instance, follow the encoding rules above, train spatial domain part of AttCRISPR with the WT-SpCas9 dataset. 
Then take the average of all spatial attention matrix, and the element in the first row and third column are closer to 1, which means when calculate the final score, 
G typically may have a important contribution at first position within sgRNA sequence. 
In fact, this corresponds to some early studies concerning the Human (hU6) promoter, which is believed to require G as the first nucleotide of its transcript \citep{cong2013multiplex,jinek2012a,mali2013rnaguided}.
\subsubsection{Method and attention in temporal domain}
As Figure~4\vphantom{\ref{fig:04}} show, temporal domain part of AttCRISPR relies on the recurrent neural network (or its variant). 
As previously mentioned, we mapping each one-hot vector into a dense real-valued high-dimensional space follow the Equation~(\ref{eq:08}), which generates the embedded matrix $X_e$.
And we regard $X_{e}$ as a sequential data, or temporal data.
Recurrent neural network (or its variant) has showed outstanding performance in the tasks with temporal data (for instance, natural language processing, sequential recommendation). 
That's why we prefer to use it to extract potential temporal features. 
To be precise, we prefer the architecture of encoder-decoder which has been proven to be effective in Seq2Seq task. 
Two main differences we have to face are that sgRNA is not a natural language in the traditional sense, and we don't have to translate it to other sequence. 
To accommodate them, the embedded matrix $X_e$ is used as input of both the encoder and decoder, and the sequence of decoder is to build the first-order preference of sgRNA sequence $\tilde{A}$. 
As mentioned above, the predicted value $y$ should satisfy Equation~(\ref{eq:05}). 

On this basis, we apply the idea of attention mechanism which has been widely used in natural language processing tasks to AttCRISPR in the method of temporal domain, 
and name it Temporal Attention Module \citep{vaswani2017attention,luong2015effective,bahdanau2014neural}.
As the idea of \citeauthor{vaswani2017attention}, Temporal Attention Module satisfies the following equation
\begin{equation}
Attention(Q,K,V)=align(Q,K)V\label{eq:13}
\end{equation}
where the $align(\cdot)$ is put forward by \citeauthor{luong2015effective}. 
$Q$, $K$, $V$ are queries, keys and values matrix accordingly in the paper of \citeauthor{vaswani2017attention}. 

As Figure~5\vphantom{\ref{fig:05}} show, in our attention module they are calculated by the following equation
\begin{equation}
\left\{\begin{array}{l}
K_i=Encoder({X_e}_i,\theta_E,K_{i-1})
\\ Q_i=Decoder({X_e}_i,\theta_D,Q_{i-1})
\\V=K 
\end{array}\right.\label{eq:14}
\end{equation}
where vector $K_i$, $Q_i$ denotes the $i$-th row of the matrix $K$ and $Q$ accordingly, $Encoder(\cdot)$ and $Decoder(\cdot)$ are independent GRU units, $\theta_E$ and $\theta_D$ denote all the related parameters of GRU networks accordingly. 
In the actual implementation, we apply the bidirectional GRU networks for better performance, and for the sake of conciseness, we show a conventional GRU network here. 
The function $align(\cdot)$ is as follows
\begin{equation}
B=align(Q,K)\label{eq:16}
\end{equation}
\begin{equation}
B_i=softmax(Q_iK^T)\otimes G_i\label{eq:17}
\end{equation}
\begin{equation}
G_{ij}=\left\{\begin{matrix}
exp(\frac{(i-j)^2}{-2\sigma})&,\left | i-j \right |\leqslant \sigma
\\ 0&,\left | i-j \right |>  \sigma
\end{matrix}\right.\label{eq:18}
\end{equation}
where the matrix $B\in\mathbb{R}^{l\times l}$ is the second-order preference we need, and vector $B_i$ denotes the $i$-th row of the matrix $B$. 
$G\in\mathbb{R}^{l\times l}$ is the damping matrix base on Gaussian function. 
Since a simple belief that the closer the base is to the $i$-th position, the more it affects the $i$-th position normally, we use the damping matrix G to constrain the network learning. 
$\sigma$ represents a threshold of length, any base over this length from the position $i$ is not considered to be affected.
Further, if we think of the values matrix as a vector form of the first-order preference $A$ in Equation~(\ref{eq:05}), we can reach the following equation
\begin{equation}
\tilde{A}=BV\label{eq:19}
\end{equation}
according to above, values matrix $V$ comes from the hidden states of a bidirectional GRU networks, which is usually hard to understand and explain.
While $B$ is the second-order preference matrix obtained by the attention mechanism, in our belief, the $j$-th dimension of $B_i$, denoted as $B_{ij}$, can reveal the effect of the base at position $j$ on position $i$ in the biological sense. 
\begin{figure}[!tpb]%figure4
    \centerline{\includegraphics[width=86mm]{temporalmodule.png}}
    \caption{The architecture of temporal domain method in AttCRISPR. 
    The input of the method is embedding sgRNA sequence $X_e$, a $21\times e$ embedding matrix, where $e$ is the dimension of a nucleotide embedding vector. 
    Then Keys $K$, Values $V$ and Queries $Q$ is generated through a classic encoder-decoder structure which is need for temporal attention module.
    Next, the temporal attention module generates the first-order preference $\tilde{A}$, a $21\times e$ matrix (or a vector set). 
    Each of the row vectors in matrix $\tilde{A}$ represents the base preference of sgRNA at the corresponding position, we use their dot product with the corresponding row vector in embedded $X_e$ to build the score of corresponding position. 
    Hence, a full connection layer is used to weighted average them and achieve the temporal output $y_t$.
    }\label{fig:04}
\centerline{\includegraphics[width=86mm]{temporalattentionmodule.png}}
\caption{        
    Details of temporal attention module. 
    To generate the first order preference vector in the $i$ position of sgRNA $\tilde{A}_i$. 
    First, the $i$-th row vector of the querys matrix $Q_i$ is multiplied by the transpose of the keys mateix $K^T$, and apply a softmax function to obtain a preliminary weights vector on the values. 
    Second, to favor the alignment points near $i$, the weights vector obtained is multiplied element-by-element by the $i$-th row vector of the damping matrix $G$, as Equation~(\ref{eq:17}) has shown. 
    $G_{ij}$ can be regarded as the result of place a Gaussian distribution centered around $i$, then sampling the position $j$ (a scaling factor is used to ensure the sum of $G_I$ is $l$). 
    Then we achieve the second-order preference matrix $B$, it is also a weights matrix on the values. So the first-order preference matrix $\tilde{A}$ come from the product of $B$ and values matrix $V$.
    Although it's not shown above, it is also worth noting that, a full connection layer with L2 constraints is used to generates a transformation matrix in order to ensure $\tilde{A}_i$ and ${X_e} _i$ are in the same vector space. 
    Temporal attention module generates the temporal first-order preference matrix $\tilde{A}$ and the temporal second-order preference matrix $B$.
    }\label{fig:05}
\end{figure}

\subsubsection{Model ensemble following stacking strategy}

As \citeauthor{wang2019optimized,Li2016ProteinSS,Wang2016ProteinSS} have shown, some indirect sgRNA features, which can't be obtained directly by deep learning, including position accessibilities of secondary structure, stemâ€“loop of secondary structure, melting temperature, and GC content are strongly associated with sgRNA activity. 
It's worth noting that in the work of \citeauthor{wang2019optimized}, hand-crafted biological features didn't be normalized. 
Since the wide range of data distribution, we believe it makes sense to normalize it, and we preprocess they based on the standard score (Z-Score).

Then we use a simple full connection network to extract the indirect features, and call the output of fully connection network $y_{bio}$. 
As mentioned above, we assume that method in spatial domain and in temporal domain are heterogeneous, which can satisfy the diversity premise of ensemble learning.
That's why we follow the stacking strategy, integrate the methods in time domain and space domain. 
Specifically, the $y_{bio}$, the spatial output $y_{s}$ and the temporal output $y_{t}$ we got earlier are concatenated and then weighted averaging is performed through a full connection layer as follow
\begin{equation}
y=W[y_{bio};y_s;y_t]\label{eq:20}
\end{equation}
where, $y$ is the final prediction value of AttCRISPR, $W$ is the weight learned by the full connection network. 
In the actual implementation, we freeze the network in the spatial domain and temporal domain at first, in order to make our network focused on learning the weight $W$.
Later in the process, in the fine tuning of AttCRISPR, the parameters of the entire network are adjusted.

\subsection{Datasets and current prediction method}

The dataset we used for training, validation and testing is built by \citeauthor{wang2019optimized} \citep{wang2019optimized}. 
We extracted 55604, 58617, 56888 sgRNAs with activity (represented by insertion/deletion (indel)) for WT-SpCas9, eSpCas9(1.1) and SpCas9-HF1 respectively, from its source data. 
As far as we know, the best performance on this dataset is the DeepHF of \citeauthor{wang2019optimized} currently, which base on deep learning and bidirectional LSTM architecture. 
DeepHF achieved Spearman correlation coefficients of 0.867, 0.862 and 0.860 for WT-SpCas9, eSpCas9(1.1) and SpCas9-HF1 respectively. 

Another tool worth noting is CRISPRpred(SEQ), which is based on conventional machine learning and need complicated human feature engineering exercise \citep{MuhammadRafid2020}. 
They also test their tool with this dataset and achieved Spearman correlation coefficients of 0.838, 0.830 and 0.821 for WT-SpCas9, eSpCas9(1.1) and SpCas9-HF1 respectively (without any hyperparameter tuning).

\citeauthor{wang2019optimized} implemente a tenfold shuffled validation to evaluate the stability of the performance of DeepHF.
To be more specific, each set is shuffled and divided into three parts, 76.5\%, 15\% and 8.5\% of the relevant data was used as the training, test and validation set respectively in a single experiment. 
The experiment is repeated ten times with the results recorded and averaged finally.
In order to make the comparison apples to apples, we will follow this strategy to design experiments.

\subsection{Experiments}

Two different sets of experiments are carried out in our work. 
The first one is designed for ablation analysis of AttCRISPR. 
We compare the performance of end2end method (without any hand-crafted biological features) in both spatial and temporal domain. 
Furthermore, we test the ensemble method based on the same strategy, which proved that the ensemble of method in both spatial and temporal domain can significantly improve the performance. 

The second experiment is designed to compare the performance of AttCRISPR and other current prediction method. 
In order to make the comparison apples to apples, we reduce the dimensionality of the same hand-crafted biological features as DeepHF's, 
which has been shown to enhance the predictability of a deep-learning model greatly, with a multilayer perceptron. 
Then follow the Equation~(\ref{eq:20}) to achieve the final prediction value. 
AttCRISPR (with the hand-crafted biological features) performs better on all three data sets than DeepHF, the current state-of-the-art. 

\begin{table}[!tpb]
    \processtable{The methods compared in our work and brief description of these methods\label{Tab:01}} 
    {\begin{tabular}{@{}lllp{3.5cm}l@{}}\toprule
        Method & Neural & End2end & Description\\\midrule
        CNN$^*$ & Yes & Yes & Naive convolutional neural network\\
        RNN$^*$ & Yes & Yes & Bidirectional long short-term memory neural network\\
        XGBoost$^*$ & No & Yes & Extreme Gradient Boosting regression tree\\
        MLP$^*$ & Yes & Yes & Multilayer perceptron\\
        DeepHF$^*$ & Yes & No & Bidirectional long short-term memory neural network (with hand-crafted biological features)\\
        CRISPRpred(SEQ)$^{\#}$ & No & No & A conventional machine learning pipeline\\
        SpAC & Yes & Yes & Spatial AttCRISPR\\
        TAC & Yes & Yes & Temporal AttCRISPR\\
        EnAC & Yes & Yes & Ensemble AttCRISPR (without hand-crafted biological features)\\
        StAC & Yes & No & Standard AttCRISPR\\
        \botrule
    \end{tabular}}\footnotesize\setlength{\parindent}{2em}{\emph{Note}: The method with superscript of $^*$ and $^{\#}$ is reported by \citeauthor{wang2019optimized} and \citeauthor{MuhammadRafid2020} respectively. 
Specially, CRISPRpred(SEQ) take another set of hand-crafted sequence-based feature to improve performance.}
\end{table}

Our baselines have a comprehensive coverage of the methods tested in these datasets. 
In Table~\ref{Tab:01}, we annotate some properties of these baselines (is/isn't neural models, is/isn't end2end models)
All the experiments were carried out in Python 3.6 using Keras 2.2.4 and one GeForce RTX 2080Ti Super was used for training and testing if needed. 

\section{Results}
\begin{figure}[!tpb]%figure4
    \centerline{\includegraphics[width=86mm]{baselinewithoutbiofeat.eps}}
    \caption{In the absence of hand-crafted biological features, performance of different algorithms for sgRNA activity prediction. 
    (a)-(c) The performance of Temporal AttCRISPR, Spatial AttCRISPR and Ensemble AttCRISPR. 
    The half-violin plots show the mean and distribution of the Spearman correlation coefficient between predicted and measured sgRNA activity scores over all tests. 
    (e)-(g) In the absence of hand-crafted biological features, the performance of all prediction methods in these datasets as far as we know. 
    The $mean \pm s.d.$ of the Spearman correlation coefficient between predicted and measured sgRNA activity scores are shown in the bar plots.}\label{fig:06}
\end{figure}
We designed experiments to address the following questions:
\begin{itemize}
    \item 
    In the absence of hand-crafted biological features, whether the stacking of method in spatial domain and temporal domain can get better performance then use these method alone? 
    $\rightarrow$ Section~\ref{section:stacking}
    \item 
    How does AttCRISPR perform compared to current state-of-the-art methods, covering both conventional machine learning and deep-learning models? 
    $\rightarrow$ Section~\ref{section:comparison}
    \item 
    How can researcher understand the decisions make by AttCRISPR locally and globally, based on attention mechanisms? 
    $\rightarrow$ Section~\ref{section:interpretability}
    \vspace*{1pt}
\end{itemize}

\subsection{Model building and stacking}\label{section:stacking}

In Table~\ref{Tab:02}, we list the performance of methods in spatial or temporal domain and the stacking of methods. 
Temporal AttCRISPR, TAC for short, achieved Spearman correlation coefficients of 0.857, 0.844, 0.851 in above three dataset. 
Spatial AttCRISPR, SpAC for short, corresponds to 0.862, 0.854, 0.857. In the absence of hand-crafted biological features. 
Ensemble AttCRISPR achieve the best performance of our knowledge, corresponds to 0.868, 0.859, 0.862. 

In addition, in Table~\ref{Tab:02}, the performance of other methods without using hand-crafted biological features, are also recorded. 
Regardless of the method we developed, RNN reported by \citeauthor{wang2019optimized}, which can be categorized as the method in temporal domain, 
is the most predictive with Spearman correlation coefficients of 0.856, 0.849, 0.851. 
It's obvious that the ensemble AttCRISPR is better at prediction (Figure~6\vphantom{\ref{fig:06}} a-c). 
Furthermore, the prediction ability of models could be boosted by addition of other hand-crafted biological features, which can't be obtained directly by sequence information. 

Hence, the second experiment is designed to compare the performance of standard AttCRISPR (hand-crafted biological features are used to improve performance of ensemble AttCRISPR)
 and current state-of-the-art method, DeepHF (the RNN mentioned above integrated with hand-crafted biological features).
\begin{table}[!tpb]
    \processtable{
    Performance comparisons for different methods in the absence of hand-crafted biological features 
    (take Spearman correlation coefficient as evaluation index)\label{Tab:02}} 
    {\begin{tabular}{@{}lccc@{}}\toprule
        Method & WT-SpCas9 & eSpCas9(1.1) & SpCas9-HF1\\\midrule
        XGBoost$^*$ & 0.845 & 0.831 & 0.818\\
        MLP$^*$ & 0.842 & 0.846 & 0.844\\
        CNN$^*$ & 0.846 & 0.831 & 0.834\\
        RNN$^*$ & 0.856 & 0.849 & 0.851\\
        TAC & 0.857 & 0.844 & 0.851\\
        SpAC & 0.862 & 0.854 & 0.857\\
        EnAC & \textbf{0.868} & \textbf{0.859} & \textbf{0.862}\\
        CRISPRpred(SEQ)$^{\#}$ & 0.838 & 0.830 & 0.821\\
        \botrule
    \end{tabular}}{}
    \processtable{
    Performance comparisons for methods before and after integrating with hand-crafted biological features
(take Spearman correlation coefficient as evaluation index)\label{Tab:03}} 
    {\begin{tabular}{lcccccc}\toprule
        & \multicolumn{2}{c}{WT-SpCas9} & \multicolumn{2}{c}{eSpCas9(1.1)} & \multicolumn{2}{c}{SpCas9-HF} \\ 
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \noalign{\smallskip} 
        Method & Mean  & \begin{tabular}[c]{@{}l@{}}Std Dev\\ ($\times 10^{-3}$)\end{tabular} & Mean & \begin{tabular}[c]{@{}l@{}}Std Dev\\ ($\times 10^{-3}$)\end{tabular} & Mean & \begin{tabular}[c]{@{}l@{}}Std Dev\\ ($\times 10^{-3}$)\end{tabular} \\ \hline
        RNN$^*$ & 0.856 & 3.33 & 0.849 & 5.00 & 0.851 & 4.11 \\
        EnAC   & 0.868 & 2.66 & 0.859 & 4.66 & 0.862 & 3.19 \\
        DeepHF$^*$ & 0.867 & \textbf{2.37} & 0.862 & 4.24 & 0.860 & 3.21 \\
        StAC   & \textbf{0.872} & 2.55 & \textbf{0.867} & \textbf{3.71} & \textbf{0.867} & \textbf{2.65} \\
     \botrule
\end{tabular}}\footnotesize\setlength{\parindent}{2em}{\emph{Note}: 
The method with superscript of $^*$ and $^{\#}$ is reported by \citeauthor{wang2019optimized} and \citeauthor{MuhammadRafid2020} respectively. 
In the tables, we use the results reported in the relevant paper as the performance of the method directly.}
\end{table}

\subsection{Comparison to current methods}\label{section:comparison}

In order to make the comparison apples to apples, 
we follow Equation~(\ref{eq:20}) to modify the ensemble method and design the control experiment using the same strategy to validate the conclusion that integrating with hand-crafted biological features can improve the predictive performance of methods. 
What's more, we compare the standard AttCRISPR and current state-of-the-art method, DeepHF, and show the results in Table~\ref{Tab:03}. 

As shown in Table~\ref{Tab:03}, in the absence of hand-crafted biological features, AttCRISPR has significant advantages over DeepHF in predictability. 
Further, integration with the hand-crafted biological features can also improve the performance of AttCRISPR, and achieve Spearman correlation coefficients of 0.872, 0.867 and 0.867 for WT-SpCas9, eSpCas9(1.1) and SpCas9-HF1 respectively. Meanwhile, current state-of-the-art method, DeepHF achieve 0.867, 0.862 and 0.860 respectively. 
After integrating with biological features, the performance gap between AttCRISPR and DeepHF is shortened, while AttCRISPR still has better performance. In addition, we also compare the standard deviation of data obtained in ten tests, which are shown in Table~\ref{Tab:03}. 
It reveal that AttCRISPR is more stable than DeepHF.

\begin{figure}[!tpb]%figure4
    \centerline{\includegraphics[width=86mm]{baselinewithbiofeat.eps}}
    \caption{
Performance comparisons for methods before and after integrating with hand-crafted biological features, 
where DeepHF is the RNN integrated with hand-crafted biological features, 
and StAC is the EnAC integrated with hand-crafted biological features. 
The box plot show the mean and distribution of Spearman correlation coefficient between predicted and measured sgRNA activity scores over all tests.
}\label{fig:07}
\end{figure}

\subsection{Interpretability of the AttCRISPR}\label{section:interpretability}

To some extent, the attention module in the AttCRISPR can help us to understand the decisions it makes. 
In the following sections, we will analyze the insight into activity of sgRNA bring through the attention mechanism at both global and local levels. 

In the following content, when we use the word, global, we are referring to rules thats are common throughout the data set. 
If assumed the method with strong generalization ability, it might mean that these rules is common in the corresponding Cas9 system.
On the other hand, local refers to the rules applicable to a certain sgRNA. 
We believe that the locally interpretable is very helpful in the design and optimization of sgRNA.
\begin{figure*}[!tpb]%figure4
    \centerline{\includegraphics[width=178mm]{spatialattention.eps}}
    \caption{wait2write}\label{fig:06}
\end{figure*}

\subsubsection{Globally interpretable}

At global level, a important question we want AttCRISPR to answer is which nucleotide it prefers at each position on the sequence. 
In fact, \citeauthor{wang2019optimized} has already answered this question in detail with the DeepSHAP method.
Leaving aside nucleotides, just consider the importance of each position, the method of \citeauthor{Liu2019} goes some way to answering this question. 

\subsubsection{Locally interpretable}

Text

\section{Discussion}

Text

\section{Conclusion}

Text

\section*{Acknowledgements}

Text\vspace*{-12pt}

\section*{Funding}

 text.\vspace*{-12pt}

\bibliographystyle{natbib}
\bibliography{document}

\end{document}